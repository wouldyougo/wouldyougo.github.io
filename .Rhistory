barplot(site.freq[order(site.freq)], horiz = T)
fbba <- c(rep("gray", 5),
rgb(59, 89, 152, maxColorValue = 255))
fbba
barplot(site.freq[order(site.freq)],
horiz = T,
col = fbba)
barplot(site.freq[order(site.freq)],
horiz = T,         # Horizontal
col = fbba,        # Use colors "fbba"
border = NA,       # No borders
xlim = c(0, 100),  # Scale from 0-100
main = "Preferred Social Networking Site\nA Survey of 202 Users",
xlab = "Number of Users")
knit("Samples R 3.Rmd")
library(knitr)
library(markdown)
knit("Samples R 3.Rmd")
markdownToHTML("Samples R 3.md", "Samples R 3.html")
library(knitr)
opts_chunk$set(fig.path='figure/silk-')
#' The report begins here.
#+ test-a, cache=FALSE
# boring examples as usual
set.seed(123)
x = rnorm(5)
mean(x)
#' You can use the special syntax {{code}} to embed inline expressions, e.g.
{{mean(x)+2}}
#' is the mean of x plus 2.
#' The code itself may contain braces, but these are not checked.  Thus,
#' perfectly valid (though very strange) R code such as `{{2 + 3}} - {{4 - 5}}`
#' can lead to errors because `2 + 3}} - {{4 - 5` will be treated as inline code.
#'
#' Now we continue writing the report. We can draw plots as well.
#+ test-b, fig.width=5, fig.height=5
par(mar = c(4, 4, .1, .1)); plot(x)
#' Actually you do not have to write chunk options, in which case knitr will use
#' default options. For example, the code below has no options attached:
var(x)
quantile(x)
#' And you can also write two chunks successively like this:
#+ test-chisq5
sum(x^2) # chi-square distribution with df 5
#+ test-chisq4
sum((x - mean(x))^2) # df is 4 now
#' Done. Call spin('knitr-spin.R') to make silk from sow's ear now and knit a
#' lovely purse.
# /* you can write comments between /* and */ like C comments (the preceding #
# is optional)
Sys.sleep(60)
library(knitr)
opts_chunk$set(fig.path='figure/silk-')
set.seed(123)
x = rnorm(5)
mean(x)
{{mean(x)+2}}
par(mar = c(4, 4, .1, .1)); plot(x)
var(x)
quantile(x)
sum(x^2) # chi-square distribution with df 5
#+ test-chisq4
sum((x - mean(x))^2) # df is 4 now
Sys.sleep(60)
# Up and Running with R
# Ex03_02
# Creating histograms for quantitative variables
# Create data frame "sn" from CSV file w/headers
sn <- read.csv("social_network.csv", header = T)
# Make a histogram of Age using the defaults
hist(sn$Age)
# Add title, colors, etc.
hist(sn$Age,
#border = NA,
col = "beige", # Or use: col = colors() [18]
main = "Ages of Respondents\nSocial Networking Survey of 202 Users",
xlab = "Age of Respondents")
# By the way, a chart of R's color palette can be found at
# http://research.stowers-institute.org/efg/R/Color/Chart/
# including a downloadable PDF version. This chart uses
# numbers for colors instead of names. To get the names, use
colors() [18]  # Which will output [1] "beige"
colors()[c(552, 254, 26)]  # Gives [1] "red" "green" "blue"
# Up and Running with R
# Ex03_03
# Creating box plots for quantitative variables
# Create data frame "sn" from CSV file w/headers
sn <- read.csv("social_network.csv", header = T)
# Make boxplots of Age and Times using the defaults
boxplot(sn$Age)
# Add title, colors, etc.
boxplot(sn$Age,
col = "beige",
notch = T,
horizontal = T,
main = "Ages of Respondents\nSocial Networking Survey of 202 Users",
xlab = "Age of Respondents")
# Up and Running with R
# Ex03_04
# Calculating frequencies
# Create data frame "sn" from CSV file w/headers
sn <- read.csv("social_network.csv", header = T)
table(sn$Site)  # Creates frequency table in alphabetical order
site.freq <- table(sn$Site)  # Saves table
site.freq  # Print table
site.freq <- site.freq[order(site.freq, decreasing = T)] # Sorts by frequency, saves table
site.freq  # Print table
prop.table(site.freq)  # Give proportions of total
round(prop.table(site.freq), 2)  # Give proportions w/2 decimal places
# Calculating descriptives
# Load data file about Google searches by state
google <- read.csv("google_correlate.csv", header = T)
names(google)
reg1 <- lm(data_viz ~
degree + stats_ed + facebook + nba + has_nba + region,
data = google)
summary(reg1)
google <- read.csv("google_correlate.csv", header = T)
google <- read.csv("google_correlate.csv", header = T)
# Up and Running with R
# Ex05_01
# Creating charts for group distributions
# Load data file about Google searches by state
google <- read.csv("google_correlate.csv", header = T)
names(google)
str(google)
# Does interest in data visualization vary by region?
# Split data by region, create new data frame
viz.reg.dist <- split(google$data_viz, google$region)
# Draw boxplots by region
boxplot(viz.reg.dist, col = "lavender")
# To draw barplot with means
viz.reg.mean <- sapply(viz.reg.dist, mean)
# Run next two together (or sequentially)
barplot(viz.reg.mean,
col = "beige",
main = "Average Google Search Share of\n\"Data Visualization\" by Region of US")
abline(h = 0)
# Install and load "psych" package to print means, etc.
install.packages("psych")
library("psych")
describeBy(google$data_viz, google$region)
# Up and Running with R
# Ex05_01
# Creating charts for group distributions
# Load data file about Google searches by state
google <- read.csv("google_correlate.csv", header = T)
names(google)
str(google)
# Does interest in data visualization vary by region?
# Split data by region, create new data frame
viz.reg.dist <- split(google$data_viz, google$region)
# Draw boxplots by region
boxplot(viz.reg.dist, col = "lavender")
# To draw barplot with means
viz.reg.mean <- sapply(viz.reg.dist, mean)
# Run next two together (or sequentially)
barplot(viz.reg.mean,
col = "beige",
main = "Average Google Search Share of\n\"Data Visualization\" by Region of US")
abline(h = 0)
# Install and load "psych" package to print means, etc.
#install.packages("psych")
library("psych")
describeBy(google$data_viz, google$region)
sn <- read.csv("social_network.csv", header = T)
install.packages("rgl")
google <- read.csv("google_correlate.csv", header = T)
install.packages("Hmisc")
sn <- read.csv("social_network.csv", header = T)
setwd("Z:/R&D/R&D Repositories/wouldyougo.github.io/how_to/Up and Running with R")
sn <- read.csv("social_network.csv", header = T)
setwd("Z:/R&D/R&D Repositories/wouldyougo.github.io/how_to")
setwd("Up and Running with R")
wd
getwd
getwd()
library("rgl")
google <- read.csv("google_correlate.csv", header = T)
names(google)
plot3d(google$data_viz,  # x variable
google$degree,    # y variable
google$facebook,  # z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
plot3d(google$data_viz,  # x variable
google$degree,    # y variable
google$facebook,  # z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
google <- read.csv("google_correlate.csv", header = T)
names(google)
g.quant <- google[c(3, 7, 4, 5)]
# Correlation matrix for data frame
cor(g.quant)
# Can test one pair of variables at a time
# Gives r, hypothesis test, and confidence interval
cor.test(g.quant$data_viz, g.quant$degree)
# Install "Hmisc" package to get p-values for matrix
#install.packages("Hmisc")
library("Hmisc")
# Need to coerce g.quant from data frame to matrix
# to get correlation matrix and p-values
rcorr(as.matrix(g.quant))
# Up and Running with R
# Up and Running with R
# Ex06_02
# Computing a regression
# Load data file about Google searches by state
google <- read.csv("google_correlate.csv", header = T)
names(google)
reg1 <- lm(data_viz ~
degree + stats_ed + facebook + nba + has_nba + region,
data = google)
summary(reg1)
# Up and Running with R
# Ex06_03
# Creating crosstabs for categorical variables
# Load data file about social networks
# Deleted one case with missing data on Gender
sn <- read.csv("social_network.csv", header = T)
names(sn)
# Create contingency table
sn.tab <- table(sn$Gender, sn$Site)
sn.tab
# If desired, can get marginal frequencies
margin.table(sn.tab, 1) # Row marginal frequencies
margin.table(sn.tab, 2) # Column marginal frequencies
# Call also get cell, row, and column %
# With rounding to get just 2 decimal places
round(prop.table(sn.tab), 2)    # cell %
round(prop.table(sn.tab, 1), 2) # row %
round(prop.table(sn.tab, 2), 2) # column %
# Chi-squared test
chisq.test(sn.tab)
# Up and Running with R
# Ex06_05
# Comparing means with ANOVA
# Load data file about Google searches by state
google <- read.csv("google_correlate.csv", header = T)
names(google)
# One Way Anova
anova1 <- aov(data_viz ~ region, data = google)
summary(anova1)
# Two Way Factorial Design
# First way to specify
anova2a <- aov(data_viz ~
region + stats_ed + region:stats_ed,
data = google)
summary(anova2a)
# Second way to specify
anova2b <- aov(data_viz ~
region*stats_ed,
data = google)
summary(anova2b)
setwd("Z:/R&D/R&D Repositories/wouldyougo.github.io/how_to")
### Пример
### add required assets such as CSS and JS libraries
```{r comment = NA, results = 'asis', message = F}
## load the package
library(rCharts)
## utility function to add required assets such as CSS and JS libraries
add_lib_assets <- function(lib, cdn = F, css = NULL) {
assets = get_assets(get_lib(lib), cdn = cdn)
if (!is.null(css)) {
assets$css = c(assets$css, css)
}
styles <- lapply(assets$css, function(style) {
sprintf("<link rel='stylesheet' href=%s>", style)
})
scripts <- lapply(assets$jshead, function(script) {
sprintf("<script type='text/javascript' src=%s></script>", script)
})
cat(paste(c(styles, scripts), collapse = "\n"))
}
# get assets from online repositories
add_lib_assets("NVD3", cdn = TRUE, css = "http://rawgithub.com/ramnathv/rCharts/master/inst/libraries/nvd3/css/rNVD3.css")
add_lib_assets("Polycharts", cdn = TRUE)
hair_eye_male <- subset(as.data.frame(HairEyeColor), Sex == "Male")
n1 <- nPlot(Freq ~ Hair, group = "Eye", data = hair_eye_male, type = "multiBarChart")
n1$print("nvd3mbar")
```
xyplot(
sp500 ~ date,
data = sp500.df,
type = "l",
main = "S&P 500 (lattice::xyplot)"
)
install.packages("xyplot")
install.packages("lattice")
install.packages("lattice")
install.packages("lattice")
xyplot(
sp500 ~ date,
data = sp500.df,
type = "l",
main = "S&P 500 (lattice::xyplot)"
)
require("lattice")
xyplot(
sp500 ~ date,
data = sp500.df,
type = "l",
main = "S&P 500 (lattice::xyplot)"
)
require(latticeExtra)
require(ggplot2)
require(reshape2)
suppressPackageStartupMessages(
require(googleVis)
)
require(PerformanceAnalytics)
require(xtsExtra)
require(rCharts)
install.packages('latticeExtra')
install.packages('ggplot2')
install.packages('reshape2')
install.packages('PerformanceAnalytics')
install.packages('xtsExtra')
require(quantmod)
require(latticeExtra)
require(ggplot2)
require(reshape2)
suppressPackageStartupMessages(require(googleVis))
require(PerformanceAnalytics)
require(quantmod)
require(xtsExtra)
require(quantmod)
# get S&P 500 data from FRED (St. Louis Fed)
sp500 <- na.omit(
getSymbols(
"SP500",
src = "FRED",
from = "1949-12-31",
auto.assign = FALSE
)
)
# use monthly data
sp500.monthly <- sp500[endpoints(sp500, on ="months")]
View(sp500)
# base plot of time series prior to xts
# get the data in data.frame format rather than xts
sp500.df <- data.frame(
index(sp500.monthly),
coredata(sp500.monthly),
stringsAsFactors=FALSE
)
# name columns
colnames( sp500.df ) <- c( "date", "sp500" )
View(sp500.df)
head(sp500.df)
View(sp500.monthly)
View(sp500)
?asTheEconomist
??asTheEconomist
require(latticeExtra)
asTheEconomist(
xyplot(
sp500.monthly,
scales = list( y = list( rot = 0 ) ),
main = "S&P 500 (lattice::xyplot.xts)"
)
)
zoo::plot.zoo(
sp500.monthly,
main = "S&P 500 (zoo::plot.zoo)"
)
require("lattice")
xyplot(
sp500 ~ date,
data = sp500.df,
type = "l",
main = "S&P 500 (lattice::xyplot)"
)
charts.PerformanceSummary(
ROC(sp500.monthly, n = 1, type = "discrete"),
main = "S&P 500 (PerformanceAnalytice::charts.PerformanceSummary)"
)
??charts.PerformanceSummary
install.packages('PerformanceAnalytics')
require('PerformanceAnalytics')
charts.PerformanceSummary(
ROC(sp500.monthly, n = 1, type = "discrete"),
main = "S&P 500 (PerformanceAnalytice::charts.PerformanceSummary)"
)
ggplot( sp500.df, aes(date) ) +
geom_line( aes( y = sp500 ) ) +
labs( title = "S&P 500 (ggplot2::ggplot)")
require(ggplot2)
ggplot( sp500.df, aes(date) ) +
geom_line( aes( y = sp500 ) ) +
labs( title = "S&P 500 (ggplot2::ggplot)")
??chartSeries
chartSeries(
sp500.monthly,
theme = chartTheme("white"),
TA = c(addBBands(),addTA(RSI(sp500.monthly)))
)
require(`quantmod`)
require(quantmod)
install.packages("timeSeries")
require(timeSeries)
require(PerformanceAnalytics)
charts.PerformanceSummary(
ROC(sp500.monthly, n = 1, type = "discrete"),
main = "S&P 500 (PerformanceAnalytice::charts.PerformanceSummary)"
)
require(PerformanceAnalytics)
# "S&P 500 (PerformanceAnalytice::charts.PerformanceSummary)"
charts.PerformanceSummary(
ROC(sp500.monthly, n = 1, type = "discrete"),
main = "S&P 500"
)
require(PerformanceAnalytics)
charts.PerformanceSummary(
ROC(sp500.monthly, n = 1, type = "discrete"),
main = "S&P 500"
)
require(googleVis)
suppressPackageStartupMessages(require(googleVis))
g1 <- gvisLineChart(
data = sp500.df,
xvar = "date",
yvar = "sp500",
options = list(
title = "S&P 500 (googleVis::gvisLineChart)",
height = 400,
width = 600
)
)
plot(g1)
suppressPackageStartupMessages(require(googleVis))
g1 <- gvisLineChart(
data = sp500.df,
xvar = "date",
yvar = "sp500",
options = list(
title = "S&P 500 (googleVis::gvisLineChart)",
height = 400,
width = 600
)
)
plot(g1)
g1
suppressPackageStartupMessages(
require(googleVis))
options(gvis.plot.tag = "chart")
M1 <- gvisMotionChart(Fruits, idvar="Fruit", timevar="Year")
plot(M1)
M1
M1 <- gvisMotionChart(Fruits, idvar="Fruit", timevar="Year")
plot(M1)
M1
g1 <- gvisLineChart(
data = sp500.df,
xvar = "date",
yvar = "sp500",
options = list(
title = "S&P 500 (googleVis::gvisLineChart)",
height = 400,
width = 600
)
)
plot(g1)
??xtsExtra
require(xtsExtra)
install.packages(xtsExtra)
install.packages('xtsExtra')
install.packages("timeSeries")
install.packages('timeSeries')
install.packages("timeSeries")
install.packages("xtsExtra", source = "http://r-forge.r-project.org")
install.packages('xtsExtra', source = "http://r-forge.r-project.org")
install.packages("Z:/R&D/Dev Programs/De_ R/Packages/xtsExtra_0.0-1.zip", repos = NULL)
require("xtsExtra")
require(xtsExtra)
require(xtsExtra)
russell2000 <- getSymbols("^RUT", from = "1900-01-01", auto.assign = F)
prices <- merge(sp500,russell2000[,4])
#use endpoints to get annual returns
returns.annual <- as.xts(
apply(
ROC(prices[endpoints(prices,"years")],type="discrete",n=1),
MARGIN = 2,
FUN = na.fill, fill = 0
),
order.by = index(prices[endpoints(prices,"years")])
)
colnames(returns.annual) <- c("S&P 500","Russell 2000")
barplot.xts(
returns.annual,
stacked=FALSE,
box="transparent",  #get rid of box surrounding the plot
ylim=c(-0.5,0.5),
ylab=NA,
border=c(brewer.pal(n=11,"BrBG")[c(4,9)]),
col=c(brewer.pal(n=11,"BrBG")[c(4,9)])
)
title(
main="Annual Returns of S&P 500 and Russell 2000 (xtsExtra::barplot.xts)",
outer = TRUE,
adj=0.05,
font.main = 1,
cex.main = 1.25,
line = -2
)
